python3 main_finetune.py --seed 0 --downstream_task classification --jitter_sigma 0.2 --rescaling_sigma 0.5 --ft_surr_phase_noise 0.075 --input_channels 1 --input_electrodes 12 --time_steps 2500 --patch_height 1 --patch_width 100 --model vit_tiny_patchX --batch_size 8 --epochs 400 --patience 25 --max_delta 0.25 --accum_iter 1 --drop_path 0.2 --weight_decay 0.2 --layer_decay 0.5 --min_lr 0.0 --blr 3e-6 --warmup_epoch 5 --smoothing 0.1 --data_path /home/nadja/ECG/data/tensors/tensor.pt --labels_path /home/nadja/ECG/data/labels/labels.pt --val_data_path /home/nadja/ECG/data/tensors/tensor.pt --val_labels_path /home/nadja/ECG/data/labels/labels.pt --nb_classes 2 --log_dir /home/nadja/ECG_new/ --num_workers 24 --finetune /home/nadja/ECG_new//MMCL-ECG-CMR/model_weights/signal_encoder_mmcl_wo_mdm.pth --pos_label 1 --global_pool --attention_pool --wandb --wandb_project MAE_ECG_Fin_Tiny_LV --output_dir /home/nadja/ECG_new//weights/
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
wandb: ERROR Error communicating with wandb process
wandb: ERROR For more info see: https://docs.wandb.ai/library/init#init-start-error
job dir: /home/nadja/ECG_new/MMCL-ECG-CMR/mae/mae
Namespace(lower_bnd=0,
upper_bnd=0,
batch_size=8,
epochs=400,
accum_iter=1,
model='vit_tiny_patchX',
input_channels=1,
input_electrodes=12,
time_steps=2500,
input_size=(1,
12,
2500),
patch_height=1,
patch_width=100,
patch_size=(1,
100),
drop_path=0.2,
masking_blockwise=False,
mask_ratio=0.0,
mask_c_ratio=0.0,
mask_t_ratio=0.0,
jitter_sigma=0.2,
rescaling_sigma=0.5,
ft_surr_phase_noise=0.075,
freq_shift_delta=0.005,
color_jitter=None,
aa='rand-m9-mstd0.5-inc1',
clip_grad=None,
weight_decay=0.2,
lr=None,
blr=3e-06,
layer_decay=0.5,
min_lr=0.0,
warmup_epochs=5,
patience=25.0,
max_delta=0.25,
smoothing=0.1,
reprob=0.25,
remode='pixel',
recount=1,
resplit=False,
mixup=0,
cutmix=0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='/home/nadja/ECG_new//MMCL-ECG-CMR/model_weights/signal_encoder_mmcl_wo_mdm.pth',
global_pool='attention_pool',
attention_pool=True,
downstream_task='classification',
data_path='/home/nadja/ECG/data/tensors/tensor.pt',
labels_path='/home/nadja/ECG/data/labels/labels.pt',
labels_mask_path='',
val_data_path='/home/nadja/ECG/data/tensors/tensor.pt',
val_labels_path='/home/nadja/ECG/data/labels/labels.pt',
val_labels_mask_path='',
nb_classes=2,
pos_label=1,
output_dir='/home/nadja/ECG_new//weights/',
log_dir='/home/nadja/ECG_new/',
wandb=True,
wandb_project='MAE_ECG_Fin_Tiny_LV',
wandb_id='',
device='cuda',
seed=0,
resume='',
plot_attention_map=False,
plot_embeddings=False,
embeddings_dir='',
predictions_dir='',
start_epoch=0,
eval=False,
dist_eval=False,
num_workers=24,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
distributed=False)
Training set size:  100
Validation set size:  100
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f7efc962ef0>
Problem at: /home/nadja/ECG_new/MMCL-ECG-CMR/mae/mae/main_finetune.py 326 main
Traceback (most recent call last):
  File "/home/nadja/ECG_new/MMCL-ECG-CMR/mae/mae/main_finetune.py", line 572, in <module>
    main(args)
  File "/home/nadja/ECG_new/MMCL-ECG-CMR/mae/mae/main_finetune.py", line 326, in main
    wandb.init(project=args.wandb_project, config=config, entity="madja161n")
  File "/home/nadja/.conda/envs/mae/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1043, in init
    run = wi.init()
  File "/home/nadja/.conda/envs/mae/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 691, in init
    raise UsageError(error_message)
wandb.errors.UsageError: Error communicating with wandb process
For more info see: https://docs.wandb.ai/library/init#init-start-error
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: Network error (ConnectTimeout), entering retry loop.
wandb: Network error (ConnectionError), entering retry loop.
